model:
  base_learning_rate: 2.0e-5 #
  target: sgm.models.diffusion.DiffusionEngine
  params:
    scale_factor: 0.18215
    disable_first_stage_autocast: True
    ckpt_path: /data/yisi/mywork/SV3D-fine-tune/cheeckpoints/sv3d_p.safetensors # this is the path for model weights with vae
    input_key: target_frames_without_noise # this is the key for the target frames

    denoiser_config:
      target: sgm.modules.diffusionmodules.denoiser.CatdiacDenoiser
      params:
        depth: 120
        scaling_config:
          target: sgm.modules.diffusionmodules.denoiser_scaling.VScalingWithEDMcNoise

    network_wrapper: sgm.modules.diffusionmodules.wrappers.OpenAIWrapper

    network_config:
      target: sgm.modules.diffusionmodules.video_model.VideoUNet # use NovelViewUnet
      params:
        adm_in_channels: 1280 # here adm_in_channels should equals to sum of vector shape[-1](cond_aug, polars_rad, azimuths_rad, height_z)
        num_classes: sequential
        use_checkpoint: True
        in_channels: 8
        out_channels: 4
        model_channels: 320
        attention_resolutions: [4, 2, 1]
        num_res_blocks: 2
        channel_mult: [1, 2, 4, 4]
        num_head_channels: 64
        use_linear_in_transformer: True
        transformer_depth: 1
        context_dim: 1024
        spatial_transformer_attn_type: softmax-xformers
        extra_ff_mix_layer: True
        use_spatial_context: True
        merge_strategy: learned_with_images
        video_kernel_size: [3, 1, 1]

    conditioner_config:
      target: sgm.modules.GeneralConditioner
      params:
        emb_models:
        - input_key: cond_frames_without_noise # crossattn
          is_trainable: False
          target: sgm.modules.encoders.modules.FrozenOpenCLIPImagePredictionEmbedder
          params:
            n_cond_frames: 1
            n_copies: 1
            open_clip_embedding_config:
              target: sgm.modules.encoders.modules.FrozenOpenCLIPImageEmbedder
              params:
                freeze: True

        - input_key: cond_frames # concat
          is_trainable: False
          target: sgm.modules.encoders.modules.VideoPredictionEmbedderWithEncoder
          params:
            disable_encoder_autocast: True
            n_cond_frames: 1
            n_copies: 1
            is_ae: True
            encoder_config:
              target: sgm.models.autoencoder.AutoencoderKLModeOnly
              params:
                embed_dim: 4
                monitor: val/rec_loss
                ddconfig:
                  attn_type: vanilla-xformers
                  double_z: True
                  z_channels: 4
                  resolution: 256
                  in_channels: 3
                  out_ch: 3
                  ch: 128
                  ch_mult: [1, 2, 4, 4]
                  num_res_blocks: 2
                  attn_resolutions: []
                  dropout: 0.0
                lossconfig:
                  target: torch.nn.Identity

        - input_key: cond_aug
          is_trainable: True
          target: sgm.modules.encoders.modules.CardiacConcatTimestepEmbedderND
          params:
            outdim: 320 # changed

        - input_key: polars_rad
          is_trainable: True
          target: sgm.modules.encoders.modules.CardiacConcatTimestepEmbedderND
          params:
            outdim: 320

        - input_key: azimuths_rad
          is_trainable: True
          target: sgm.modules.encoders.modules.CardiacConcatTimestepEmbedderND
          params:
            outdim: 320

        # Train this embedderND
        - input_key: height_z
          is_trainable: True
          target: sgm.modules.encoders.modules.CardiacConcatTimestepEmbedderND
          params:
            outdim: 320

    first_stage_config:
      target: sgm.models.autoencoder.AutoencodingEngine
      params:
        loss_config:
          target: torch.nn.Identity
        regularizer_config:
          # target: torch.nn.Identity # Debugging
          target: sgm.modules.autoencoding.regularizers.DiagonalGaussianRegularizer
        #CHECK HERE
        # mutiply the encoder_config because we need to encode the first stage, choose this one if we have merged the safetensor 
        # or this one, if we have already prepared all the latents by vae
        encoder_config:
          target: torch.nn.Identity
        decoder_config:
          target: sgm.modules.diffusionmodules.model.Decoder
          params:
            attn_type: vanilla-xformers
            double_z: True
            z_channels: 4
            resolution: 256
            in_channels: 3
            out_ch: 3
            ch: 128
            ch_mult: [ 1, 2, 4, 4 ]
            num_res_blocks: 2
            attn_resolutions: [ ]
            dropout: 0.0

    loss_fn_config:
      target: sgm.modules.diffusionmodules.loss.CardiacDiffusionLoss
      params:
        loss_weighting_config:
          target: sgm.modules.diffusionmodules.loss_weighting.VWeighting
        sigma_sampler_config:
          target: sgm.modules.diffusionmodules.sigma_sampling.EDMSampling
          params:
            p_mean: 0.7
            p_std: 1.6
        batch2model_keys: ["image_only_indicator", "num_video_frames"]

    sampler_config:
      target: sgm.modules.diffusionmodules.sampling.EulerEDMSampler
      params:
        verbose: True
        num_steps: 50

        discretization_config:
          target: sgm.modules.diffusionmodules.discretizer.EDMDiscretization
          params:
            sigma_max: 700.0

        guider_config:
          target: sgm.modules.diffusionmodules.guiders.TrianglePredictionGuider
          params:
            num_frames: 120
            max_scale: 2.5

data:
  target: sgm.data.simple.SV3DCardiacDataModuleFromConfig
  params:
    root_dir: '/data/yisi/mywork/SV3D-fine-tune/input_data/04ed'
    batch_size: 1 ### maybe now we could only support batch_size=1
    num_workers: 0 ### check
    total_view: 120
    train:
      validation: False
      image_transforms:
        size: 576

    validation:
      validation: True
      image_transforms:
        size: 576

lightning: # lightning_config
  modelcheckpoint:
    params:
      save_on_train_epoch_end: False
      every_n_train_steps: 500
      
  trainer: # trainer_config
    devices: 0,1,2,3
    benchmark: True
    num_sanity_val_steps: 0
    accumulate_grad_batches: 1
    max_epochs: 10
    precision: '16-mixed'